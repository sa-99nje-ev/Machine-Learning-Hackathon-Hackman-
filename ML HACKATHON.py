# -*- coding: utf-8 -*-
"""Untitled23-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lipyB5hwklcY-orJb0W4cnwdcauh-0vA
"""

import numpy as np
import pandas as pd
import random
import pickle
from collections import defaultdict, Counter
import string
from itertools import product
import re

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

class HangmanHMM:
    def __init__(self):
        self.transition_probs = {}
        self.emission_probs = {}
        self.initial_probs = {}
        self.alphabet = set(string.ascii_lowercase)

    def train(self, corpus_words):
        """Train HMM on corpus words"""
        # Group words by length for better modeling
        words_by_length = defaultdict(list)
        for word in corpus_words:
            words_by_length[len(word)].append(word)

        # Train separate models for each word length
        self.models_by_length = {}

        for length, words in words_by_length.items():
            if len(words) < 10:  # Skip lengths with too few examples
                continue

            model = self._train_length_specific_model(words, length)
            self.models_by_length[length] = model

    def _train_length_specific_model(self, words, length):
        """Train HMM for specific word length"""
        # Count transitions and emissions
        transition_counts = defaultdict(lambda: defaultdict(int))
        emission_counts = defaultdict(lambda: defaultdict(int))
        initial_counts = defaultdict(int)

        for word in words:
            # Initial state (first letter)
            initial_counts[word[0]] += 1

            # Emissions (position -> letter)
            for pos, letter in enumerate(word):
                emission_counts[pos][letter] += 1

            # Transitions (letter to next letter)
            for i in range(len(word) - 1):
                transition_counts[word[i]][word[i+1]] += 1

        # Convert counts to probabilities with smoothing
        model = {
            'initial_probs': self._normalize_with_smoothing(initial_counts),
            'emission_probs': {pos: self._normalize_with_smoothing(emissions)
                             for pos, emissions in emission_counts.items()},
            'transition_probs': {state: self._normalize_with_smoothing(transitions)
                               for state, transitions in transition_counts.items()},
            'length': length
        }

        return model

    def _normalize_with_smoothing(self, counts, alpha=0.01):
        """Normalize counts to probabilities with Laplace smoothing"""
        total = sum(counts.values()) + alpha * len(self.alphabet)
        probs = {}

        for letter in self.alphabet:
            probs[letter] = (counts.get(letter, 0) + alpha) / total

        return probs

    def get_letter_probabilities(self, masked_word, guessed_letters):
        """Get probability distribution for next letter to guess"""
        word_length = len(masked_word)

        if word_length not in self.models_by_length:
            # Fallback to frequency analysis
            return self._frequency_fallback(masked_word, guessed_letters)

        model = self.models_by_length[word_length]
        letter_probs = defaultdict(float)

        # Use forward algorithm to get position-specific probabilities
        for pos, char in enumerate(masked_word):
            if char == '_':  # Unknown position
                for letter in self.alphabet:
                    if letter not in guessed_letters:
                        # Probability based on position and context
                        prob = model['emission_probs'][pos].get(letter, 1e-6)

                        # Context from adjacent known letters
                        if pos > 0 and masked_word[pos-1] != '_':
                            prev_letter = masked_word[pos-1]
                            prob *= model['transition_probs'].get(prev_letter, {}).get(letter, 1e-6)

                        if pos < len(masked_word) - 1 and masked_word[pos+1] != '_':
                            next_letter = masked_word[pos+1]
                            prob *= model['transition_probs'].get(letter, {}).get(next_letter, 1e-6)

                        letter_probs[letter] += prob

        # Normalize probabilities
        total_prob = sum(letter_probs.values())
        if total_prob > 0:
            for letter in letter_probs:
                letter_probs[letter] /= total_prob

        return dict(letter_probs)

    def _frequency_fallback(self, masked_word, guessed_letters):
        """Fallback method using letter frequency"""
        common_letters = ['e', 't', 'a', 'o', 'i', 'n', 's', 'h', 'r', 'd', 'l', 'u']
        probs = {}

        for i, letter in enumerate(common_letters):
            if letter not in guessed_letters:
                probs[letter] = (len(common_letters) - i) / len(common_letters)

        # Add remaining letters with low probability
        for letter in self.alphabet:
            if letter not in guessed_letters and letter not in probs:
                probs[letter] = 0.01

        return probs

class HangmanEnvironment:
    def __init__(self, word_list):
        self.word_list = word_list
        self.reset()

    def reset(self, word=None):
        """Reset environment with new word"""
        if word is None:
            self.target_word = random.choice(self.word_list).lower()
        else:
            self.target_word = word.lower()

        self.masked_word = ['_'] * len(self.target_word)
        self.guessed_letters = set()
        self.wrong_guesses = 0
        self.max_wrong = 6
        self.game_over = False
        self.won = False

        return self.get_state()

    def step(self, letter):
        """Make a guess and return new state, reward, done"""
        letter = letter.lower()

        # Check for repeated guess
        if letter in self.guessed_letters:
            return self.get_state(), -2, self.game_over  # Penalty for repeated guess

        self.guessed_letters.add(letter)

        # Check if letter is in word
        if letter in self.target_word:
            # Reveal all instances of the letter
            for i, char in enumerate(self.target_word):
                if char == letter:
                    self.masked_word[i] = letter

            # Check if word is complete
            if '_' not in self.masked_word:
                self.won = True
                self.game_over = True
                reward = 100  # Large reward for winning
            else:
                reward = 10  # Small reward for correct guess
        else:
            self.wrong_guesses += 1
            reward = -5  # Penalty for wrong guess

            if self.wrong_guesses >= self.max_wrong:
                self.game_over = True
                reward = -50  # Large penalty for losing

        return self.get_state(), reward, self.game_over

    def get_state(self):
        """Get current state representation"""
        return {
            'masked_word': ''.join(self.masked_word),
            'guessed_letters': self.guessed_letters.copy(),
            'wrong_guesses': self.wrong_guesses,
            'lives_left': self.max_wrong - self.wrong_guesses,
            'game_over': self.game_over,
            'won': self.won
        }

class HangmanQLearningAgent:
    def __init__(self, hmm_model, learning_rate=0.1, discount=0.95, epsilon=0.1):
        self.hmm = hmm_model
        # Use regular dict instead of defaultdict for proper serialization
        self.q_table = {}
        self.learning_rate = learning_rate
        self.discount = discount
        self.epsilon = epsilon
        self.alphabet = set(string.ascii_lowercase)

    def get_state_key(self, state):
        """Convert state to hashable key for Q-table"""
        masked = state['masked_word']
        guessed = ''.join(sorted(state['guessed_letters']))
        lives = state['lives_left']
        return f"{masked}:{guessed}:{lives}"

    def get_q_value(self, state_key, action):
        """Get Q-value with proper initialization"""
        if state_key not in self.q_table:
            self.q_table[state_key] = {}
        if action not in self.q_table[state_key]:
            self.q_table[state_key][action] = 0.0
        return self.q_table[state_key][action]

    def set_q_value(self, state_key, action, value):
        """Set Q-value with proper initialization"""
        if state_key not in self.q_table:
            self.q_table[state_key] = {}
        self.q_table[state_key][action] = value

    def choose_action(self, state, training=True):
        """Choose action using epsilon-greedy policy"""
        available_letters = self.alphabet - state['guessed_letters']

        if not available_letters:
            return None

        state_key = self.get_state_key(state)

        # Epsilon-greedy exploration
        if training and random.random() < self.epsilon:
            return random.choice(list(available_letters))

        # Get HMM probabilities
        hmm_probs = self.hmm.get_letter_probabilities(
            state['masked_word'], state['guessed_letters']
        )

        # Combine Q-values with HMM probabilities
        best_letter = None
        best_score = float('-inf')

        for letter in available_letters:
            q_value = self.get_q_value(state_key, letter)
            hmm_prob = hmm_probs.get(letter, 0.01)

            # Combined score: Q-value + HMM probability
            combined_score = q_value + hmm_prob * 10

            if combined_score > best_score:
                best_score = combined_score
                best_letter = letter

        return best_letter

    def update_q_value(self, state, action, reward, next_state):
        """Update Q-value using Q-learning update rule"""
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)

        # Get max Q-value for next state
        available_next = self.alphabet - next_state['guessed_letters']
        if available_next and not next_state['game_over']:
            max_next_q = max(self.get_q_value(next_state_key, letter)
                           for letter in available_next)
        else:
            max_next_q = 0

        # Q-learning update
        current_q = self.get_q_value(state_key, action)
        new_q = current_q + self.learning_rate * (
            reward + self.discount * max_next_q - current_q
        )
        self.set_q_value(state_key, action, new_q)

    def decay_epsilon(self, decay_rate=0.995):
        """Decay exploration rate"""
        self.epsilon = max(0.01, self.epsilon * decay_rate)

def train_agent(agent, env, episodes=10000):
    """Train the RL agent"""
    scores = []
    win_rates = []
    episode_rewards = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        while not state['game_over']:
            action = agent.choose_action(state, training=True)
            if action is None:
                break

            next_state, reward, done = env.step(action)
            agent.update_q_value(state, action, reward, next_state)

            total_reward += reward
            state = next_state

        episode_rewards.append(total_reward)
        scores.append(1 if state['won'] else 0)

        # Track performance
        if episode % 1000 == 0:
            recent_wins = sum(scores[-1000:]) if len(scores) >= 1000 else sum(scores)
            win_rate = recent_wins / min(1000, len(scores))
            win_rates.append(win_rate)
            print(f"Episode {episode}: Win rate = {win_rate:.3f}, Epsilon = {agent.epsilon:.3f}")

        # Decay exploration
        if episode % 100 == 0:
            agent.decay_epsilon()

    return scores, win_rates, episode_rewards

def evaluate_agent(agent, env, test_words, num_games=2000):
    """Evaluate agent performance"""
    total_games = 0
    wins = 0
    total_wrong_guesses = 0
    total_repeated_guesses = 0

    results = []

    for i, word in enumerate(test_words[:num_games]):
        state = env.reset(word)
        game_wrong = 0
        game_repeated = 0
        guesses_made = set()

        while not state['game_over']:
            action = agent.choose_action(state, training=False)
            if action is None:
                break

            if action in guesses_made:
                game_repeated += 1
            guesses_made.add(action)

            next_state, reward, done = env.step(action)

            if reward == -5:  # Wrong guess
                game_wrong += 1

            state = next_state

        total_games += 1
        if state['won']:
            wins += 1

        total_wrong_guesses += game_wrong
        total_repeated_guesses += game_repeated

        results.append({
            'word': word,
            'won': state['won'],
            'wrong_guesses': game_wrong,
            'repeated_guesses': game_repeated
        })

    success_rate = wins / total_games
    avg_wrong = total_wrong_guesses / total_games
    avg_repeated = total_repeated_guesses / total_games

    # Calculate final score using the provided formula
    final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)

    return {
        'success_rate': success_rate,
        'total_wrong_guesses': total_wrong_guesses,
        'total_repeated_guesses': total_repeated_guesses,
        'avg_wrong_guesses': avg_wrong,
        'avg_repeated_guesses': avg_repeated,
        'final_score': final_score,
        'detailed_results': results
    }

def main():
    print("Loading data...")

    # Load corpus and test data
    with open('corpus.txt', 'r') as f:
        corpus_words = [word.strip().lower() for word in f.read().split()]

    with open('test.txt', 'r') as f:
        test_words = [word.strip().lower() for word in f.read().split()]

    print(f"Corpus: {len(corpus_words)} words")
    print(f"Test set: {len(test_words)} words")

    # Train HMM
    print("Training HMM...")
    hmm = HangmanHMM()
    hmm.train(corpus_words)

    # Create environment and agent
    env = HangmanEnvironment(corpus_words)
    agent = HangmanQLearningAgent(hmm)

    # Train agent
    print("Training RL agent...")
    scores, win_rates, episode_rewards = train_agent(agent, env, episodes=15000)

    # Evaluate on test set
    print("Evaluating on test set...")
    results = evaluate_agent(agent, env, test_words, num_games=2000)

    print(f"\n=== FINAL RESULTS ===")
    print(f"Success Rate: {results['success_rate']:.2%}")
    print(f"Total Wrong Guesses: {results['total_wrong_guesses']}")
    print(f"Total Repeated Guesses: {results['total_repeated_guesses']}")
    print(f"Average Wrong Guesses per Game: {results['avg_wrong_guesses']:.2f}")
    print(f"Average Repeated Guesses per Game: {results['avg_repeated_guesses']:.2f}")
    print(f"Final Score: {results['final_score']:.1f}")

    # Save models with proper serialization
    print("Saving models...")
    try:
        with open('hangman_hmm.pkl', 'wb') as f:
            pickle.dump(hmm, f)

        with open('hangman_agent.pkl', 'wb') as f:
            pickle.dump(agent, f)

        print("✓ Models saved successfully!")

        # Test loading the models
        with open('hangman_hmm.pkl', 'rb') as f:
            loaded_hmm = pickle.load(f)

        with open('hangman_agent.pkl', 'rb') as f:
            loaded_agent = pickle.load(f)

        print("✓ Models loaded successfully for verification!")

    except Exception as e:
        print(f"Error saving models: {e}")
        print("Continuing without saving...")

    return results

if __name__ == "__main__":
    results = main()

# --------------------------------
# --------------------------------

import matplotlib.pyplot as plt
import numpy as np

# Add this cell after the main training is complete
# Create visualization of training progress and performance

def plot_training_metrics(scores, win_rates, episode_rewards):
    """Create comprehensive training visualization"""

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # 1. Win Rate Over Time
    episodes = range(0, len(win_rates) * 1000, 1000)
    ax1.plot(episodes, win_rates, 'b-', linewidth=2, marker='o', markersize=4)
    ax1.set_title('Win Rate During Training', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Win Rate')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, max(win_rates) * 1.1)

    # 2. Cumulative Win Rate (smoothed)
    window_size = 100
    cumulative_wins = np.cumsum(scores)
    episodes_full = np.arange(1, len(scores) + 1)
    cumulative_rate = cumulative_wins / episodes_full

    ax2.plot(episodes_full[::window_size], cumulative_rate[::window_size], 'g-', linewidth=2)
    ax2.set_title('Cumulative Win Rate', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Cumulative Win Rate')
    ax2.grid(True, alpha=0.3)

    # 3. Episode Rewards Distribution
    reward_bins = np.linspace(min(episode_rewards), max(episode_rewards), 30)
    ax3.hist(episode_rewards, bins=reward_bins, alpha=0.7, color='orange', edgecolor='black')
    ax3.set_title('Distribution of Episode Rewards', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Total Reward per Episode')
    ax3.set_ylabel('Frequency')
    ax3.grid(True, alpha=0.3)

    # 4. Learning Progress (Moving Average)
    window = 500
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')
        ax4.plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2)
        ax4.set_title(f'Moving Average Reward (Window: {window})', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Average Reward')
        ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_performance_analysis(results):
    """Create performance analysis visualization"""

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # Extract data from results
    detailed = results['detailed_results']
    words = [r['word'] for r in detailed]
    wins = [r['won'] for r in detailed]
    wrong_guesses = [r['wrong_guesses'] for r in detailed]
    repeated_guesses = [r['repeated_guesses'] for r in detailed]

    # 1. Word Length vs Success Rate
    word_lengths = [len(w) for w in words]
    length_performance = {}

    for i, length in enumerate(word_lengths):
        if length not in length_performance:
            length_performance[length] = []
        length_performance[length].append(wins[i])

    lengths = sorted(length_performance.keys())
    success_rates = [np.mean(length_performance[l]) for l in lengths]
    counts = [len(length_performance[l]) for l in lengths]

    bars = ax1.bar(lengths, success_rates, alpha=0.7, color='skyblue', edgecolor='navy')
    ax1.set_title('Success Rate by Word Length', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Word Length')
    ax1.set_ylabel('Success Rate')
    ax1.grid(True, alpha=0.3, axis='y')

    # Add count labels on bars
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'n={count}', ha='center', va='bottom', fontsize=10)

    # 2. Wrong Guesses Distribution
    ax2.hist(wrong_guesses, bins=range(0, max(wrong_guesses)+2), alpha=0.7,
             color='coral', edgecolor='darkred')
    ax2.set_title('Distribution of Wrong Guesses per Game', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Number of Wrong Guesses')
    ax2.set_ylabel('Frequency')
    ax2.grid(True, alpha=0.3)

    # 3. Win/Loss Analysis
    win_count = sum(wins)
    loss_count = len(wins) - win_count

    labels = ['Wins', 'Losses']
    sizes = [win_count, loss_count]
    colors = ['lightgreen', 'lightcoral']
    explode = (0.05, 0)  # explode the wins slice

    ax3.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
            shadow=True, startangle=90)
    ax3.set_title('Overall Win/Loss Ratio', fontsize=14, fontweight='bold')

    # 4. Performance Metrics Summary
    metrics = [
        f"Success Rate: {results['success_rate']:.1%}",
        f"Avg Wrong Guesses: {results['avg_wrong_guesses']:.2f}",
        f"Avg Repeated Guesses: {results['avg_repeated_guesses']:.2f}",
        f"Final Score: {results['final_score']:.1f}",
        f"Total Games: {len(detailed)}"
    ]

    ax4.axis('off')
    ax4.text(0.1, 0.8, 'Performance Summary', fontsize=16, fontweight='bold',
             transform=ax4.transAxes)

    for i, metric in enumerate(metrics):
        ax4.text(0.1, 0.6 - i*0.1, metric, fontsize=12,
                transform=ax4.transAxes)

    # Add a performance grade
    score = results['final_score']
    if score > 0:
        grade = "Excellent"
        color = "green"
    elif score > -10000:
        grade = "Good"
        color = "orange"
    else:
        grade = "Needs Improvement"
        color = "red"

    plt.tight_layout()
    plt.show()

# Usage: Add this at the end of your main() function, before the return statement
if 'scores' in locals() and 'win_rates' in locals() and 'episode_rewards' in locals():
    print("\nGenerating training visualizations...")
    plot_training_metrics(scores, win_rates, episode_rewards)

if 'results' in locals():
    print("Generating performance analysis...")
    plot_performance_analysis(results)